<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linux on Utop&#39;s Blog</title>
    <link>https://ffutop.github.io/tags/linux/</link>
    <description>Recent content in Linux on Utop&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>2018-2019 © ffutop</copyright>
    <lastBuildDate>Mon, 16 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://ffutop.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>系统平均负载分析</title>
      <link>https://ffutop.github.io/posts/2019-09-16-load-average/</link>
      <pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-09-16-load-average/</guid>
      <description>&lt;p&gt;&lt;code&gt;Load Average&lt;/code&gt; 是监控系统负载的重要指标。但是，在最近的测试中，使用简单的 CPU 密集型程序执行 1 分钟，系统 1 分钟的平均负载却只能达到 0.63。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ffutop $ time ./a
./a  59.48s user 0.04s system 98% cpu 1:00.59 total
ffutop $ uptime
09:58:56 up 165 days, 20:14,  2 users,  load average: 0.63, 0.24, 0.09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;是什么原因导致平均负载与预期值不符呢？&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Linux Traffic Control</title>
      <link>https://ffutop.github.io/posts/2019-08-23-traffic-control/</link>
      <pubDate>Fri, 23 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-08-23-traffic-control/</guid>
      <description>&lt;p&gt;最近在处理 Kubernetes 工作的时候，被问及这样一个命题：Pod 能对 CPU 和内存施加限制，那同样属于资源范畴的网络带宽是否能限制呢？使用 Kubernetes 的一个核心优势在于每个 Pod 都等同于一个轻量级的“操作系统”。建立在 Linux 命名空间(namespace)和控制组(cgroups)基础上的容器技术将每个 Pod 的资源进行了隔离和限制。但是，限流只针对 CPU 和内存，对网络、磁盘 IO 的解决方案仅仅局限在隔离，难道技术上实现不了吗？自然不是，Kubernetes 有意识地将网络模块拆解，只定义插件规范，而将实现的可能性交由下游开发自由决策。当然，本篇并不在意 Kubernetes 网络限流的解决方案，只是以此作为引子。&lt;/p&gt;

&lt;p&gt;流量控制(Traffic Control, TC) 是 Linux 内核提供的流量限速、整形、策略控制机制。近乎完美地支持网络限流的命题，除了，这是比 Netfilter 更难理解的模块。Netfilter 作用在内核网络协议栈上，通过在各个枢纽设立关卡对网络包(&lt;code&gt;sk_buff&lt;/code&gt; 数据结构, skb)进行检查，并实施 ACCEPT、DROP、MASQUERADE 等策略。相比之下，TC 是绑定在网络设备上实施的。提供 &lt;code&gt;enqueue&lt;/code&gt;, &lt;code&gt;dequeue&lt;/code&gt; 两个核心函数，也是作为关卡对到达网络设备的网络包实施策略。要说核心的不同之处，Netfilter 是流式地处理网络包，先到的网络包一定先出（也可能是被丢弃）；TC 的处理方式就依照策略，类比块设备的随机读/随机写了。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Netfilter 导览 - based on iptables</title>
      <link>https://ffutop.github.io/posts/2019-08-06-netfilter/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-08-06-netfilter/</guid>
      <description>&lt;p&gt;&lt;code&gt;Netfilter&lt;/code&gt;，内核中用于管理网络数据包的重要网络模块；&lt;code&gt;iptables&lt;/code&gt;，运行在用户空间，用于控制 &lt;code&gt;Netfilter&lt;/code&gt; 模块的一种软件。需要注意的，&lt;code&gt;iptables&lt;/code&gt; 只能控制 IPv4 数据包，对于 IPv6 数据包，需要使用 &lt;code&gt;ip6tables&lt;/code&gt; 。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (13) - 虚拟内存</title>
      <link>https://ffutop.github.io/posts/2019-07-17-understand-kernel-13/</link>
      <pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-07-17-understand-kernel-13/</guid>
      <description>&lt;p&gt;几乎每个进程都有独立的虚拟地址空间，这是一个逻辑上的概念，用于建立进程对进程存储资源的认知。对于 32 位机，虚拟地址空间的大小通常是 4GB；对于 64 位机，最大可以达到 $2^{64}$ Bytes 。&lt;/p&gt;

&lt;p&gt;本篇便是为了看看虚拟地址空间究竟如何被内核管理，又是怎样和物理内存、文件等资源关联。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (12) - Linux 容器化技术</title>
      <link>https://ffutop.github.io/posts/2019-06-18-understand-kernel-12/</link>
      <pubDate>Tue, 18 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-06-18-understand-kernel-12/</guid>
      <description>&lt;p&gt;由于工作上的一些调整，目前开始接触容器化技术了。容器化技术相对于虚拟机的最大区别，在于其只是在操作系统上做了资源隔离和控制，而虚拟机则会基于原有的操作系统，模拟一整套硬件设备接口，运行一个新的操作系统及相关的 Lib 库。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://img.ffutop.com/9FD2522D-BBEB-443A-8267-26F1EC77BA87.png&#34; alt=&#34;Containerd VS VM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;small&gt;&lt;center&gt;Copied From docker.com&lt;/center&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;如何实现容器化，显然需要从操作系统层面进行支撑。这其中涉及到的核心技术，就包括命名空间(namespace)和控制组(Control Group, cgroup)，前者用来对资源进行隔离，后者用来对资源加以限制。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;本文所有涉及的内核代码基于版本 3.10.1 ；所有命令执行结果基于 Ubuntu 18.04&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>跟踪内核函数的工具—— Ftrace</title>
      <link>https://ffutop.github.io/posts/2019-06-02-ftrace/</link>
      <pubDate>Sun, 02 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-06-02-ftrace/</guid>
      <description>&lt;p&gt;前两天仿照 &lt;code&gt;strings&lt;/code&gt; 工具写了个打印进程运行时堆栈可打印字符的工具 &lt;a href=&#34;https://github.com/DorMOUSE-None/ff-proc-utils/blob/master/ffstrings.c&#34;&gt;ffstrings&lt;/a&gt; 。结果没几天就被告知在 CentOS 上跑不通(uid: root, errno: EPERM):&amp;lt;&lt;/p&gt;

&lt;p&gt;在反复调试无果之后，希望得到一种可以跟踪内核执行流的工具。功夫不负有心人，在内核文档中找到了—— &lt;code&gt;ftrace&lt;/code&gt; 。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Ftrace 是位于内核内部的跟踪器，可以用来调试和分析发生在用户空间（内核空间）之外的延迟和性能问题。&lt;/p&gt;

&lt;p&gt;虽然 Ftrace 是一款函数跟踪器，但也支持基于其它目的的跟踪，例如：跟踪上下文切换、跟踪高优先级任务的执行时间等等。另外还可以通过插件的方式自定义更多的跟踪。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当然，目前仅仅是做简单的介绍。毕竟最初的目的是为了确认 &lt;code&gt;ffstrings&lt;/code&gt; 出现 &lt;code&gt;EPERM&lt;/code&gt; 的原因。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (11) - 进程间通信</title>
      <link>https://ffutop.github.io/posts/2019-05-27-understand-kernel-11/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-05-27-understand-kernel-11/</guid>
      <description>&lt;p&gt;进程间通信(IPC, inter-process communication)是多个执行上下文实现数据交互的重要功能，也是 Linux Kernel 一个重要的模块。本篇主要着眼于 Linux 基于 System V 引入的 3 种 IPC 机制——信号量、消息队列、共享内存。除此之外，Linux 还有更多的方式能够实现进程间通信，但本文不做介绍。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;本篇基于 Linux 4.9.87 版本源码&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel(10) - Context of Execution</title>
      <link>https://ffutop.github.io/posts/2019-04-10-understand-kernel-10/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-04-10-understand-kernel-10/</guid>
      <description>&lt;p&gt;在进行&lt;a href=&#34;https://www.ffutop.com/2018-10-12-understand-Kernel-4/&#34;&gt;第四篇(任务调度)&lt;/a&gt;行文描述时，就一直闹不清内核所谓的&lt;code&gt;task&lt;/code&gt;的概念。之前一直将其与进程(process)的概念等同视之。但这又导致了线程的概念无处安置（毕竟在计算机科学的概念中，线程作为进程的子集存在，负责程序执行）。不过，现在这个疑惑总算得到了合理的解释：&lt;strong&gt;我们错误地将理论和实践不加区分地混淆了&lt;/strong&gt;。内核开发社区与学术界的合作在整个内核开发历史上并没有想象中的频繁，正相反，学术界对内核代码的贡献不到1%[1]。如果想要将进程/线程的思想代入内核，并逐一印证，那么过程将非常痛苦并最终一无所获。所谓进程/线程，在内核中只有一个概念——执行的上下文(Context of Execution)，任何想要对进程/线程概念进行区分的行为都将是作茧自缚[2]。同时，&lt;code&gt;task&lt;/code&gt; 也就是 &lt;code&gt;Context of Execution&lt;/code&gt; 概念在实现上的表征。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>如何获取运行时进程堆栈</title>
      <link>https://ffutop.github.io/posts/2019-03-25-mem-dump/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-03-25-mem-dump/</guid>
      <description>&lt;p&gt;前些天看了关于在密码学应用中使用&lt;code&gt;java.lang.String&lt;/code&gt;与&lt;code&gt;byte[]&lt;/code&gt;的相关讨论，不推荐使用&lt;code&gt;java.lang.String&lt;/code&gt;的重点就是其将在JVM中驻留，从而可能被窃取。但是，如何从内存中获取这些内容？JVM当然提供了一些机制，但是个人更喜欢从内核的角度来看看这个问题。&lt;/p&gt;

&lt;h2 id=&#34;proc-pid-maps&#34;&gt;/proc/${pid}/maps&lt;/h2&gt;

&lt;p&gt;首先当然是确定进程堆栈在物理内存的位置啦。很遗憾，没有找到相关的方案。毕竟进程记录的都是虚拟线性地址，而通过内核分段、分页机制最终映射到物理内存。不过，从&lt;code&gt;/proc&lt;/code&gt;虚拟文件系统中，提供了进程虚拟地址映射。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;address                   perm offset   dev   inode                      pathname
556566cb5000-556566cb6000 r-xp 00000000 fc:01 2496598                    /root/ffTrace/run
556566eb5000-556566eb6000 r--p 00000000 fc:01 2496598                    /root/ffTrace/run
556566eb6000-556566eb7000 rw-p 00001000 fc:01 2496598                    /root/ffTrace/run
55656814f000-556568170000 rw-p 00000000 00:00 0                          [heap]
7f2a95f91000-7f2a96178000 r-xp 00000000 fc:01 1835434                    /lib/x86_64-linux-gnu/libc-2.27.so
7f2a96178000-7f2a96378000 ---p 001e7000 fc:01 1835434                    /lib/x86_64-linux-gnu/libc-2.27.so
7f2a96378000-7f2a9637c000 r--p 001e7000 fc:01 1835434                    /lib/x86_64-linux-gnu/libc-2.27.so
7f2a9637c000-7f2a9637e000 rw-p 001eb000 fc:01 1835434                    /lib/x86_64-linux-gnu/libc-2.27.so
7f2a9637e000-7f2a96382000 rw-p 00000000 00:00 0
7f2a96382000-7f2a963a9000 r-xp 00000000 fc:01 1835410                    /lib/x86_64-linux-gnu/ld-2.27.so
7f2a965a0000-7f2a965a2000 rw-p 00000000 00:00 0
7f2a965a9000-7f2a965aa000 r--p 00027000 fc:01 1835410                    /lib/x86_64-linux-gnu/ld-2.27.so
7f2a965aa000-7f2a965ab000 rw-p 00028000 fc:01 1835410                    /lib/x86_64-linux-gnu/ld-2.27.so
7f2a965ab000-7f2a965ac000 rw-p 00000000 00:00 0
7ffe2cf5e000-7ffe2cf7f000 rw-p 00000000 00:00 0                          [stack]
7ffe2cfed000-7ffe2cff0000 r--p 00000000 00:00 0                          [vvar]
7ffe2cff0000-7ffe2cff2000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (9) - IO Multiplexing</title>
      <link>https://ffutop.github.io/posts/2019-03-05-understand-kernel-9/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-03-05-understand-kernel-9/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;./2019-01-15-understand-Kernel-8/&#34;&gt;前一篇&lt;/a&gt;已经对 Linux 内核网络相关的内容进行了基础性的介绍。数据从到达网卡，到最终被用户进程获取，最少经过了三个进程/硬中断的配合：网络中断负责将网络接收到的数据读取到内存并添加到 softnet_data 队列，并设置软中断通知内核进程 ksoftirqd；内核进程 ksoftirqd 被调度并处于运行状态，处理位于 softnet_data 中的 &lt;code&gt;struct sock&lt;/code&gt; 对象，将其逐级从网络接口层逐级提升到网络层、传输层&amp;hellip;最终添加到接收队列 &lt;code&gt;sk_receive_queue&lt;/code&gt; 中；用户进程通过 &lt;code&gt;read&lt;/code&gt;、&lt;code&gt;recv&lt;/code&gt;、&lt;code&gt;recvfrom&lt;/code&gt; 等命令检查并获取 &lt;code&gt;sk_receive_queue&lt;/code&gt; 中的数据。&lt;/p&gt;

&lt;p&gt;整个流程从概述上可以很轻松地配合进行网络数据交互，但如果要监控多个网络套接字呢？处理流程将变得复杂。我们无法预知哪个套接字能优先接收到数据。因此，最直接的办法就是轮询，在用户程序硬编码，通过设置超时时间的方式尝试获取数据。当然，这个效率就相当低下了。每次试探都需要触发系统调用（要知道这代价可是相当大的），另外超时时间的设置也是一个硬性的阻塞式消耗。&lt;/p&gt;

&lt;p&gt;那么，有没有解决方案呢？当然有。通过用户程序硬编码式的轮询显然是陷入性能瓶颈的根源。因此内核主动提供了轮询式的系统调用（&lt;code&gt;select&lt;/code&gt;, &lt;code&gt;poll&lt;/code&gt;, &lt;code&gt;epoll&lt;/code&gt;）。通过将轮询逻辑下沉到内核态，系统调用就只会有一次，而且超时时间的设置也显得统一。本篇就要就 &lt;code&gt;select&lt;/code&gt; 和 &lt;code&gt;epoll&lt;/code&gt; 两类系统调用的实现进行探究。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (8) - 网络</title>
      <link>https://ffutop.github.io/posts/2019-01-15-understand-kernel-8/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2019-01-15-understand-kernel-8/</guid>
      <description>PDF</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (7) - 字符设备</title>
      <link>https://ffutop.github.io/posts/2018-12-28-understand-kernel-7/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-12-28-understand-kernel-7/</guid>
      <description>&lt;p&gt;相比较于块设备，字符设备无论从物理认知上，抑或是理论理解上，都存在着相当大的入门门槛。特别是在将字符设备与控制台、命令行终端混淆的时候，就更加难以进行分辨了。&lt;/p&gt;

&lt;p&gt;回到字符设备本身，字符设备与块设备最主要的区别就在于块设备可以随机读写，而字符设备只能够顺序读，顺序写。&lt;/p&gt;

&lt;p&gt;那么，常见的字符设备有什么？显示器、键盘、鼠标。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (6) - read &amp; write</title>
      <link>https://ffutop.github.io/posts/2018-11-11-understand-kernel-6/</link>
      <pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-11-11-understand-kernel-6/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://dormouse-none.github.io/2018-10-14-understand-Kernel-5/&#34;&gt;前一篇&lt;/a&gt;已经描述对文件系统进行了宏观性的描述，这一篇，将以特定的文件读写操作为示例，串联对整个文件系统的基本操作。&lt;/p&gt;

&lt;p&gt;首先先来看看平台相关的文件读写操作的 C 代码是怎样一个调用方式&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-c&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;errno.h&amp;gt;
#include &amp;lt;sys/types.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;

int panic()
{
    fprintf(stderr, &amp;quot;%s (errno=%d)\n&amp;quot;, strerror(errno), errno);
    return -1;
}

int main(int argc, char *argv[])
{
    /* 打开文件 frw.txt (以可读写 | 若不存在则新建的形式) */
    int fd = open(&amp;quot;/root/frw.txt&amp;quot;, O_RDWR | O_CREAT);
    if (fd == -1)
        return panic();

    /* 向文件写入 Hello World! 共计 12 个字符 */
    ssize_t wsize = write(fd, &amp;quot;Hello World!&amp;quot;, 12);
    if (wsize == -1)
        return panic();

    /* 重定位文件读写指针 */
    off_t off = lseek(fd, 0, SEEK_SET);
    if (off == -1)
        return panic();

    char* buf = (char *) malloc(wsize);
    /* 读取文件内容 */
    ssize_t rsize = read(fd, buf, wsize);
    if (rsize == -1)
        return panic();

    printf(&amp;quot;%s\n&amp;quot;, buf);
    free(buf);
    /* 关闭文件 */
    int stat = close(fd);
    if (stat == -1)
        return panic();

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (5) - 文件系统(宏观描述)</title>
      <link>https://ffutop.github.io/posts/2018-10-14-understand-kernel-5/</link>
      <pubDate>Sun, 14 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-10-14-understand-kernel-5/</guid>
      <description>&lt;p&gt;用惯了类 Unix 系统，应该说文件系统是日常最常接触的一个操作系统模块之一了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ls
Applications Network      Users        bin          data         etc          net          sbin         usr
Library      System       Volumes      cores        dev          home         private      tmp          var
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是，究竟什么是文件系统? 为什么需要文件系统? 难道文件不是简单地存储到存储设备一块连续区域的吗?&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (4) - 任务调度</title>
      <link>https://ffutop.github.io/posts/2018-10-12-understand-kernel-4/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-10-12-understand-kernel-4/</guid>
      <description>&lt;p&gt;前面几节已经描述过，对于单核 CPU 来说。CPU 就处于不断地执行指令的过程(或者通过 &lt;code&gt;hlt&lt;/code&gt; 指令直接停止工作)。&lt;/p&gt;

&lt;p&gt;针对于每一个程序来说，这个程序执行流程是通过 CPU 中几组寄存器(通用寄存器、段寄存器、控制寄存器等) 和存储在内存中的代码和数据协作完成的。&lt;/p&gt;

&lt;p&gt;如果要达到单核多任务的目的，首先要做的就是完成对几组寄存器中当前值的保存(我称之为保存现场)。而对于内存来说，多个任务的代码、数据同时存在内存是完全合理且可行的。毕竟相较于有限的寄存器，内存实在是太大了(相对而言)。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (3) - 操作系统启动</title>
      <link>https://ffutop.github.io/posts/2018-10-06-understand-kernel-3/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-10-06-understand-kernel-3/</guid>
      <description>&lt;p&gt;这次拖得有够久的，毕竟需要将知识串联起来并不是一件容易的事情。更何况很多内容可以说和常理(个人理解的常理)有了比较大的偏差。&lt;/p&gt;

&lt;p&gt;不过确实比较有意思。从引导程序到操作系统启动，这中间究竟经历了多少流程呢？&lt;/p&gt;

&lt;p&gt;由于前几篇已经有过介绍，这里不会再对引导程序及汇编语法做过多的介绍。而着重描述整个操作系统的启动流程。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (2) - 多任务切换</title>
      <link>https://ffutop.github.io/posts/2018-08-26-understand-kernel-2/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-08-26-understand-kernel-2/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;

&lt;p&gt;《只是为了好玩》书中，林纳斯描述过他最早的试验性程序就是执行两个不同的任务（一个不断输出A，另一个输出B），同时不断地让 CPU 在两个任务间做切换。结合《Linux 内核完全注释》提供的一个多任务切换示例程序，本篇将就多任务切换程序的执行流程进行详述，并提供当下汇编工具下的适配。&lt;/p&gt;

&lt;p&gt;关于运行环境的说明，欢迎参考 &lt;a href=&#34;https://www.ffutop.com/2018-08-19-understand-Kernel-1/#Bochs-%E4%BB%BF%E7%9C%9F%E5%99%A8&#34;&gt;Bochs 仿真器使用简介&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (0) - 概述</title>
      <link>https://ffutop.github.io/posts/2018-08-19-understand-kernel-0/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-08-19-understand-kernel-0/</guid>
      <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;

&lt;p&gt;《理解Linux Kernel》系列最早开始于 2018 年中，当时我刚刚结束对 JVM (Java Virtual Machine, Java 虚拟机) &lt;a href=&#34;https://docs.oracle.com/javase/specs/jvms/se9/html/jvms-4.html&#34;&gt;ClassFile 文件格式&lt;/a&gt;的学习。彼时，在*实现新编程语言*、*学习 Linux 内核实现*、*继续深入 JVM 源码*三个下一阶段的命题中，我选择了拥抱 Linux 源码。&lt;/p&gt;

&lt;p&gt;时间过去了大半年，我已经简单地建立了对 Linux Kernel 的结构化认知。趁着现阶段有一些闲暇，整理过去的文章，以期删繁就简，并对错误的描述进行修正。&lt;/p&gt;

&lt;p&gt;《理解 Linux Kernel》系列最早发布于&lt;a href=&#34;https://www.ffutop.com/&#34;&gt;我的博客 (Utop&amp;rsquo;s Blog)&lt;/a&gt;。由于这些文章的完成完全出于个人兴趣，对每个内核子系统的学习往往浅尝辄止，但我依旧认为这个系列对初学者来说是大有裨益的。&lt;/p&gt;

&lt;p&gt;本系列对内核的学习完全建立在 0.11、2.6.24 两个版本源码的基础上。从 0.11 版本入门，学习硬件启动阶段进行的主要操作、任务调度、文件系统等早期版本已初现雏形的子系统；从 2.6.24 版本进阶，学习网络、内存管理等子系统的实现。在此强烈推荐，直接阅读源码对理解的帮助最为深远。请随时备上两个版本的源码，以备深入理解。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>理解 Linux Kernel (1) - BIOS</title>
      <link>https://ffutop.github.io/posts/2018-08-19-understand-kernel-1/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://ffutop.github.io/posts/2018-08-19-understand-kernel-1/</guid>
      <description>&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;

&lt;p&gt;在&lt;a href=&#34;https://www.ffutop.com/2018-08-19-understand-Kernel-0/&#34;&gt;概述&lt;/a&gt;，我已经介绍过《理解 Linux Kernel》系列文章的写作原因。我不能担保我所进行的所有试验性操作都是对的，但至少操作我的环境下成功地运行了，并帮助我触及我始终敬畏的&lt;strong&gt;硬件&amp;amp;OS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;《Linux 内核完全注释》第三章——内核编程语言和环境，描述了用 as86 汇编语言构建 boot 引导程序，在 Bochs 仿真器成功模拟开机运行，最终输出 *Loading System&amp;hellip;*。这就是本篇所要尝试的核心实验。之所以在已经有资料的基础上再写一遍，是书中缺失了仿真器模拟的环节。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>