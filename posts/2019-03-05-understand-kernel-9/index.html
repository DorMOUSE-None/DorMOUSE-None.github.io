<!DOCTYPE html>
<html lang="zh-cn">
<head>
  
    <title>理解 Linux Kernel (9) - IO Multiplexing :: Utop&#39;s Blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="前一篇已经对 Linux 内核网络相关的内容进行了基础性的介绍。数据从到达网卡，到最终被用户进程获取，最少经过了三个进程/硬中断的配合：网络中断负责将网络接收到的数据读取到内存并添加到 softnet_data 队列，并设置软中断通知内核进程 ksoftirqd；内核进程 ksoftirqd 被调度并处于运行状态，处理位于 softnet_data 中的 struct sock 对象，将其逐级从网络接口层逐级提升到网络层、传输层&amp;hellip;最终添加到接收队列 sk_receive_queue 中；用户进程通过 read、recv、recvfrom 等命令检查并获取 sk_receive_queue 中的数据。
整个流程从概述上可以很轻松地配合进行网络数据交互，但如果要监控多个网络套接字呢？处理流程将变得复杂。我们无法预知哪个套接字能优先接收到数据。因此，最直接的办法就是轮询，在用户程序硬编码，通过设置超时时间的方式尝试获取数据。当然，这个效率就相当低下了。每次试探都需要触发系统调用（要知道这代价可是相当大的），另外超时时间的设置也是一个硬性的阻塞式消耗。
那么，有没有解决方案呢？当然有。通过用户程序硬编码式的轮询显然是陷入性能瓶颈的根源。因此内核主动提供了轮询式的系统调用（select, poll, epoll）。通过将轮询逻辑下沉到内核态，系统调用就只会有一次，而且超时时间的设置也显得统一。本篇就要就 select 和 epoll 两类系统调用的实现进行探究。
"/>
<meta name="keywords" content="BLOG"/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://ffutop.github.io/posts/2019-03-05-understand-kernel-9/" />


<link rel="stylesheet" href="https://ffutop.github.io/assets/style.css">

  <link rel="stylesheet" href="https://ffutop.github.io/assets/blue.css">






<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://ffutop.github.io/img/apple-touch-icon-144-precomposed.png">

<link rel="shortcut icon" href="https://ffutop.github.io/favicon.ico">



<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="理解 Linux Kernel (9) - IO Multiplexing :: Utop&#39;s Blog — " />
<meta name="twitter:description" content="前一篇已经对 Linux 内核网络相关的内容进行了基础性的介绍。数据从到达网卡，到最终被用户进程获取，最少经过了三个进程/硬中断的配合：网络中断负责将网络接收到的数据读取到内存并添加到 softnet_data 队列，并设置软中断通知内核进程 ksoftirqd；内核进程 ksoftirqd 被调度并处于运行状态，处理位于 softnet_data 中的 struct sock 对象，将其逐级从网络接口层逐级提升到网络层、传输层&amp;hellip;最终添加到接收队列 sk_receive_queue 中；用户进程通过 read、recv、recvfrom 等命令检查并获取 sk_receive_queue 中的数据。
整个流程从概述上可以很轻松地配合进行网络数据交互，但如果要监控多个网络套接字呢？处理流程将变得复杂。我们无法预知哪个套接字能优先接收到数据。因此，最直接的办法就是轮询，在用户程序硬编码，通过设置超时时间的方式尝试获取数据。当然，这个效率就相当低下了。每次试探都需要触发系统调用（要知道这代价可是相当大的），另外超时时间的设置也是一个硬性的阻塞式消耗。
那么，有没有解决方案呢？当然有。通过用户程序硬编码式的轮询显然是陷入性能瓶颈的根源。因此内核主动提供了轮询式的系统调用（select, poll, epoll）。通过将轮询逻辑下沉到内核态，系统调用就只会有一次，而且超时时间的设置也显得统一。本篇就要就 select 和 epoll 两类系统调用的实现进行探究。
" />
<meta name="twitter:site" content="https://ffutop.github.io/" />
<meta name="twitter:creator" content="fangfeng" />
<meta name="twitter:image" content="">


<meta property="og:locale" content="zh-cn" />
<meta property="og:type" content="article" />
<meta property="og:title" content="理解 Linux Kernel (9) - IO Multiplexing :: Utop&#39;s Blog — ">
<meta property="og:description" content="前一篇已经对 Linux 内核网络相关的内容进行了基础性的介绍。数据从到达网卡，到最终被用户进程获取，最少经过了三个进程/硬中断的配合：网络中断负责将网络接收到的数据读取到内存并添加到 softnet_data 队列，并设置软中断通知内核进程 ksoftirqd；内核进程 ksoftirqd 被调度并处于运行状态，处理位于 softnet_data 中的 struct sock 对象，将其逐级从网络接口层逐级提升到网络层、传输层&amp;hellip;最终添加到接收队列 sk_receive_queue 中；用户进程通过 read、recv、recvfrom 等命令检查并获取 sk_receive_queue 中的数据。
整个流程从概述上可以很轻松地配合进行网络数据交互，但如果要监控多个网络套接字呢？处理流程将变得复杂。我们无法预知哪个套接字能优先接收到数据。因此，最直接的办法就是轮询，在用户程序硬编码，通过设置超时时间的方式尝试获取数据。当然，这个效率就相当低下了。每次试探都需要触发系统调用（要知道这代价可是相当大的），另外超时时间的设置也是一个硬性的阻塞式消耗。
那么，有没有解决方案呢？当然有。通过用户程序硬编码式的轮询显然是陷入性能瓶颈的根源。因此内核主动提供了轮询式的系统调用（select, poll, epoll）。通过将轮询逻辑下沉到内核态，系统调用就只会有一次，而且超时时间的设置也显得统一。本篇就要就 select 和 epoll 两类系统调用的实现进行探究。
" />
<meta property="og:url" content="https://ffutop.github.io/posts/2019-03-05-understand-kernel-9/" />
<meta property="og:site_name" content="理解 Linux Kernel (9) - IO Multiplexing" />
<meta property="og:image" content="">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

<meta property="article:published_time" content="2019-03-05 00:00:00 &#43;0000 UTC" />










<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92258941-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "UA-92258941-1");
</script>



</head>
<body class="">


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    ffutop
  </div>
</a>

    </div>
    <div class="menu-trigger">menu</div>
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">关于</a></li>
        
      
        
          <li><a href="/archives">归档</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">关于</a></li>
      
    
      
        <li><a href="/archives">归档</a></li>
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://ffutop.github.io/posts/2019-03-05-understand-kernel-9/">理解 Linux Kernel (9) - IO Multiplexing</a></h1>
  <div class="post-meta">
      
    <span class="post-date">
      2019-03-05
    </span>
    
    
    <span class="post-author">::
      fangfeng
    </span>
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://ffutop.github.io/tags/linux/">Linux</a>&nbsp;
    
    #<a href="https://ffutop.github.io/tags/kernel/">Kernel</a>&nbsp;
    
    #<a href="https://ffutop.github.io/tags/io-model/">IO Model</a>&nbsp;
    
  </span>
  

  

  <div class="post-content">
    <p><a href="./2019-01-15-understand-Kernel-8/">前一篇</a>已经对 Linux 内核网络相关的内容进行了基础性的介绍。数据从到达网卡，到最终被用户进程获取，最少经过了三个进程/硬中断的配合：网络中断负责将网络接收到的数据读取到内存并添加到 softnet_data 队列，并设置软中断通知内核进程 ksoftirqd；内核进程 ksoftirqd 被调度并处于运行状态，处理位于 softnet_data 中的 <code>struct sock</code> 对象，将其逐级从网络接口层逐级提升到网络层、传输层&hellip;最终添加到接收队列 <code>sk_receive_queue</code> 中；用户进程通过 <code>read</code>、<code>recv</code>、<code>recvfrom</code> 等命令检查并获取 <code>sk_receive_queue</code> 中的数据。</p>

<p>整个流程从概述上可以很轻松地配合进行网络数据交互，但如果要监控多个网络套接字呢？处理流程将变得复杂。我们无法预知哪个套接字能优先接收到数据。因此，最直接的办法就是轮询，在用户程序硬编码，通过设置超时时间的方式尝试获取数据。当然，这个效率就相当低下了。每次试探都需要触发系统调用（要知道这代价可是相当大的），另外超时时间的设置也是一个硬性的阻塞式消耗。</p>

<p>那么，有没有解决方案呢？当然有。通过用户程序硬编码式的轮询显然是陷入性能瓶颈的根源。因此内核主动提供了轮询式的系统调用（<code>select</code>, <code>poll</code>, <code>epoll</code>）。通过将轮询逻辑下沉到内核态，系统调用就只会有一次，而且超时时间的设置也显得统一。本篇就要就 <code>select</code> 和 <code>epoll</code> 两类系统调用的实现进行探究。</p>

<h2 id="select">SELECT</h2>

<p><code>select</code> 是一种比较古老的系统调用方式，存在着许多的调用限制。当然，相对的也就比较容易理解。</p>

<p><code>select</code> 系统调用最初的逻辑就是对超时时间的相关操作，完成从用户数据段到内核数据段的数据拷贝工作，而使用 <code>copy_from_user</code> 的目的，虽然看似比较晦涩，不如大家在用户程序中普通调用的memcpy，但究其原因，是为了处理MMU相关的问题。将内核复制的数据timeval做一定的取整操作（因为硬件中断的精度可能达不到timeval所描述的粒度，故根据HZ做取整，恰好为时钟中断周期的整数倍）。调用 <code>core_sys_select</code> 进行真正的轮询操作（当然，好像也算不上真正的逻辑，还有一层核心的调用呢）。结束后的操作当然是准备更新timeval。因为进行轮询的结果有两种，一种是因为超时而结束 <code>core_sys_select</code> 函数调用；另一种是得到了一个或多个准备就绪的数据，才结束调用并返回。显然，因为第二种结果而更新timeval有着一定的实际意义——到底等待了多久。不过这里需要注意的是，不是所有的Linux系统都支持对timeval做更新。</p>

<pre><code class="language-c">asmlinkage long sys_select(int n, fd_set __user *inp, fd_set __user *outp,
			fd_set __user *exp, struct timeval __user *tvp)
{
	s64 timeout = -1;
	struct timeval tv;
	int ret;

    /* 如果设置了超时时间 */
	if (tvp) {
		if (copy_from_user(&amp;tv, tvp, sizeof(tv)))
			return -EFAULT;

        /* 计时器不能设置为负数 */
		if (tv.tv_sec &lt; 0 || tv.tv_usec &lt; 0)
			return -EINVAL;

		/* Cast to u64 to make GCC stop complaining */
        /* 对 timeout 做一定的处理，根据时钟周期对 usec 进行取整 */
		if ((u64)tv.tv_sec &gt;= (u64)MAX_INT64_SECONDS)
			timeout = -1;	/* 无限等待 */
		else {
			timeout = DIV_ROUND_UP(tv.tv_usec, USEC_PER_SEC/HZ);
			timeout += tv.tv_sec * HZ;
		}
	}

    /* 核心的 select 实现逻辑 */
	ret = core_sys_select(n, inp, outp, exp, &amp;timeout);

    /* 如果设置了超时时间 */
	if (tvp) {
		struct timeval rtv;

		if (current-&gt;personality &amp; STICKY_TIMEOUTS)
			goto sticky;
		rtv.tv_usec = jiffies_to_usecs(do_div((*(u64*)&amp;timeout), HZ));
		rtv.tv_sec = timeout;
		if (timeval_compare(&amp;rtv, &amp;tv) &gt;= 0)
			rtv = tv;
        /* 内核数据-&gt;用户数据的拷贝，更新距离超时剩余的时间间隔 */
		if (copy_to_user(tvp, &amp;rtv, sizeof(rtv))) {
sticky:
			/*
             * 如果应用程序把timeval放在只读内存中，内核就无需特别的更新它
			 * If an application puts its timeval in read-only
			 * memory, we don't want the Linux-specific update to
			 * the timeval to cause a fault after the select has
			 * completed successfully. However, because we're not
			 * updating the timeval, we can't restart the system
			 * call.
			 */
			if (ret == -ERESTARTNOHAND)
				ret = -EINTR;
		}
	}

	return ret;
}
</code></pre>

<p><code>core_sys_select</code> 也没有太多核心操作，完全是在为三个位图——输入、输出、错误从用户数据段到内核数据段的拷贝工作，以及在结束 <code>do_select</code> 调用后的反向拷贝操作。不过，无论是 <code>sys_select</code> 还是 <code>core_sys_select</code> 都将各自需要完成的工作分割得非常清晰，一个完成timeval的处理工作，另一个完成位图的处理。</p>

<pre><code class="language-c">/*
 * 内核数据-位图的准备，调用do_select完成核心的逻辑
 */
static int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp,
			   fd_set __user *exp, s64 *timeout)
{
	fd_set_bits fds;
	void *bits;
	int ret, max_fds;
	unsigned int size;
	struct fdtable *fdt;
    /*
     * 在栈中定义较小的数组，减少堆内存的占用，同时可以更快；
     * SELECT_STACK_ALLOC = 256
     */
	long stack_fds[SELECT_STACK_ALLOC/sizeof(long)];

	ret = -EINVAL;
	if (n &lt; 0)
		goto out_nofds;

	/* max_fds can increase, so grab it once to avoid race */
    /* max_fds 可能同时在增加，所以提前获取一份固定的拷贝避免竞争 */
	rcu_read_lock();
    /* 获取当前任务的文件描述符表 */
	fdt = files_fdtable(current-&gt;files);
	max_fds = fdt-&gt;max_fds;
	rcu_read_unlock();
	if (n &gt; max_fds)
		n = max_fds;

	/*
	 * We need 6 bitmaps (in/out/ex for both incoming and outgoing),
	 * since we used fdset we need to allocate memory in units of
	 * long-words.
	 */
	size = FDS_BYTES(n);
	bits = stack_fds;
	if (size &gt; sizeof(stack_fds) / 6) {
		/* Not enough space in on-stack array; must use kmalloc */
        /* 栈数组大小不足；使用 kmalloc 获取新的空间 */
		ret = -ENOMEM;
		bits = kmalloc(6 * size, GFP_KERNEL);
		if (!bits)
			goto out_nofds;
	}
	fds.in      = bits;
	fds.out     = bits +   size;
	fds.ex      = bits + 2*size;
	fds.res_in  = bits + 3*size;
	fds.res_out = bits + 4*size;
	fds.res_ex  = bits + 5*size;

    /* 用户数据段的数据 inp、outp、exp 往内核数据段 fds.xxx 拷贝 */
	if ((ret = get_fd_set(n, inp, fds.in)) ||
	    (ret = get_fd_set(n, outp, fds.out)) ||
	    (ret = get_fd_set(n, exp, fds.ex)))
		goto out;
    /* fds.res_xxx 作为 do_select 的执行结果，预置为0，表示每一位相应的fd都未准备就绪 */
	zero_fd_set(n, fds.res_in);
	zero_fd_set(n, fds.res_out);
	zero_fd_set(n, fds.res_ex);

    /* 交由 do_select 完成真正核心的操作 */
	ret = do_select(n, &amp;fds, timeout);

	if (ret &lt; 0)
		goto out;
	if (!ret) {
		ret = -ERESTARTNOHAND;
		if (signal_pending(current))
			goto out;
		ret = 0;
	}

    /* 将结果集拷贝回用户数据段的 inp、outp、exp 中 */
	if (set_fd_set(n, inp, fds.res_in) ||
	    set_fd_set(n, outp, fds.res_out) ||
	    set_fd_set(n, exp, fds.res_ex))
		ret = -EFAULT;

out:
	if (bits != stack_fds)
		kfree(bits);
out_nofds:
	return ret;
}
</code></pre>

<p>终于是核心的逻辑了，逐一地将当前任务挂到相应文件的等待队列上，并等待调用 <code>poll</code> 函数被唤醒</p>

<pre><code class="language-c">int do_select(int n, fd_set_bits *fds, s64 *timeout)
{
	struct poll_wqueues table;
	poll_table *wait;
	int retval, i;

	rcu_read_lock();
    /* 确认fds.xxx所需轮询的fd全部全部处于打开状态，同时返回最大的fd */
	retval = max_select_fd(n, fds);
	rcu_read_unlock();

	if (retval &lt; 0)
		return retval;
	n = retval;

    /* 把当前任务放入自己的等待队列中 */
	poll_initwait(&amp;table);
	wait = &amp;table.pt;
    /* 如果超时时间为0，即无需等待 */
	if (!*timeout)
		wait = NULL;
	retval = 0;
    /* 无限循环 */
	for (;;) {
		unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp;
		long __timeout;

		set_current_state(TASK_INTERRUPTIBLE);

		inp = fds-&gt;in; outp = fds-&gt;out; exp = fds-&gt;ex;
		rinp = fds-&gt;res_in; routp = fds-&gt;res_out; rexp = fds-&gt;res_ex;

		for (i = 0; i &lt; n; ++rinp, ++routp, ++rexp) {
			unsigned long in, out, ex, all_bits, bit = 1, mask, j;
			unsigned long res_in = 0, res_out = 0, res_ex = 0;
			const struct file_operations *f_op = NULL;
			struct file *file = NULL;

			in = *inp++; out = *outp++; ex = *exp++;
			all_bits = in | out | ex;
			if (all_bits == 0) {
				i += __NFDBITS;
				continue;
			}

            /* 对 unsigned long 的每一位进行确认 */
			for (j = 0; j &lt; __NFDBITS; ++j, ++i, bit &lt;&lt;= 1) {
				int fput_needed;
                /* 超过需检测的最大的文件描述符 */
				if (i &gt;= n)
					break;
                /* 该 fd 无需检测，直接下一个 */
				if (!(bit &amp; all_bits))
					continue;
                /* 获取相应的文件实例 */
				file = fget_light(i, &amp;fput_needed);
				if (file) {
					f_op = file-&gt;f_op;
					mask = DEFAULT_POLLMASK;
                    /* 对于套接字，调用的是sock_poll，在poll成功时将唤醒wait队列的任务（即把当前任务唤醒）*/
					if (f_op &amp;&amp; f_op-&gt;poll)
						mask = (*f_op-&gt;poll)(file, retval ? NULL : wait);
					fput_light(file, fput_needed);
					if ((mask &amp; POLLIN_SET) &amp;&amp; (in &amp; bit)) {
						res_in |= bit;
						retval++;
					}
					if ((mask &amp; POLLOUT_SET) &amp;&amp; (out &amp; bit)) {
						res_out |= bit;
						retval++;
					}
					if ((mask &amp; POLLEX_SET) &amp;&amp; (ex &amp; bit)) {
						res_ex |= bit;
						retval++;
					}
				}
                /* 主动让出CPU片，等待重新调度（提前设置了最高优先级PREEMPT_ACTIVE）*/
				cond_resched();
			}
			if (res_in)
				*rinp = res_in;
			if (res_out)
				*routp = res_out;
			if (res_ex)
				*rexp = res_ex;
		}
		wait = NULL;
		if (retval || !*timeout || signal_pending(current))
			break;
		if(table.error) {
			retval = table.error;
			break;
		}

		if (*timeout &lt; 0) {
			/* Wait indefinitely */
			__timeout = MAX_SCHEDULE_TIMEOUT;
		} else if (unlikely(*timeout &gt;= (s64)MAX_SCHEDULE_TIMEOUT - 1)) {
			/* Wait for longer than MAX_SCHEDULE_TIMEOUT. Do it in a loop */
			__timeout = MAX_SCHEDULE_TIMEOUT - 1;
			*timeout -= __timeout;
		} else {
			__timeout = *timeout;
			*timeout = 0;
		}
        /* 进入延时唤醒状态，待定预定的超时时间 */
		__timeout = schedule_timeout(__timeout);
		if (*timeout &gt;= 0)
			*timeout += __timeout;
	}
	__set_current_state(TASK_RUNNING);

	poll_freewait(&amp;table);

	return retval;
}
</code></pre>

<p>总结来看，<code>select</code> 完全能够支持IO多路复用。至少比用户程序自行实现轮询优秀得多。但是，也存在着一些明显的缺陷：</p>

<ol>
<li>支持的文件描述符存在上限，默认是1024。</li>
<li>每次陷入内核态 <code>select</code> 函数之后，都需要按位遍历所有的文件描述符（无论该fd是否存在），max(fd)越大，开销越大。</li>
<li>每次调用 <code>select</code> 都需要将fd集合从用户态复制到内核态，max(fd)越大，开销越大。</li>
</ol>

<h2 id="epoll">EPOLL</h2>

<p>简单地描述过 <code>select</code> 系统调用之后，我们着重来聊一聊 <code>epoll</code> 的实现。毕竟 <code>select</code> 和 <code>poll</code> 的复杂度是 $O(N)$，而 <code>epoll</code> 只是 $O(\log{N})$ （当然，这里对时间复杂度的比较维度不同，稍候细讲）。<code>epoll</code> 区别于传统I/O复用模型的最大特色在于：它将创建并维护一个 <code>eventpoll</code> 实例，并通过注册请 <code>epoll</code> 对新的文件描述符进行监听，这意味着数据从用户数据区到内核数据区的拷贝只有一次；相对的，传统方式在每次轮询时，都需要全量地将数据从用户数据区拷贝到内核数据区。</p>

<h3 id="epoll-create"><code>epoll_create</code></h3>

<p><code>epoll_create</code> 负责创建一个新的 <code>eventpoll</code> 实例。这里的size并没有实际意义（由于历史原因而存在），传入的参数将被忽略。看源码总是件有意思的事，这个size被描述成了检查精神健全的标志&hellip;</p>

<pre><code class="language-c">asmlinkage long sys_epoll_create(int size)
{
	int error, fd = -1;
	struct eventpoll *ep;
	struct inode *inode;
	struct file *file;

	DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_create(%d)\n&quot;,
		     current, size));

	/*
     * 精神健全检测(size)；同时建立内部数据结构 struct eventpoll
	 */
	error = -EINVAL;
	if (size &lt;= 0 || (error = ep_alloc(&amp;ep)) != 0)
		goto error_return;

	/*
     * 创建一个新的文件描述符，文件数据结构和i节点
	 */
	error = anon_inode_getfd(&amp;fd, &amp;inode, &amp;file, &quot;[eventpoll]&quot;,
				 &amp;eventpoll_fops, ep);
	if (error)
		goto error_free;

	DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_create(%d) = %d\n&quot;,
		     current, size, fd));

	return fd;

error_free:
	ep_free(ep);
error_return:
	DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_create(%d) = %d\n&quot;,
		     current, size, error));
	return error;
}
</code></pre>

<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0qi7lrtgzj31eu0u075i.jpg" alt="Epoll Create | Model" /></p>

<h3 id="epoll-ctl"><code>epoll_ctl</code></h3>

<p><code>epoll_ctl</code> 顾名思义——<code>epoll</code>控制器，用于增加、修改、删除监听的事件。这里 <code>epfd</code> 用于找到 <code>eventpoll</code> 实例，<code>fd</code> 表示需要监听的文件描述符，<code>op</code> 区分增删改，<code>event</code> 表示监听的具体事件描述。</p>

<pre><code class="language-c">asmlinkage long sys_epoll_ctl(int epfd, int op, int fd,
			      struct epoll_event __user *event)
{
	int error;
	struct file *file, *tfile;
	struct eventpoll *ep;
	struct epitem *epi;
	struct epoll_event epds;

	DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_ctl(%d, %d, %d, %p)\n&quot;,
		     current, epfd, op, fd, event));

	error = -EFAULT;
    /* 如果是增改操作，需要把事件描述拷贝到内核数据区; 删除操作不需要 */
	if (ep_op_has_event(op) &amp;&amp;
	    copy_from_user(&amp;epds, event, sizeof(struct epoll_event)))
		goto error_return;

	/* Get the &quot;struct file *&quot; for the eventpoll file */
	error = -EBADF;
	file = fget(epfd);
	if (!file)
		goto error_return;

	/* Get the &quot;struct file *&quot; for the target file */
	tfile = fget(fd);
	if (!tfile)
		goto error_fput;

    /* 需要监听的文件描述符必须支持文件操作 poll */
	error = -EPERM;
	if (!tfile-&gt;f_op || !tfile-&gt;f_op-&gt;poll)
		goto error_tgt_fput;

    /* 需要确保不能把epfd作为被监听的fd加入 */
	error = -EINVAL;
	if (file == tfile || !is_file_epoll(file))
		goto error_tgt_fput;

    /* 
     * 这里的文件描述符一定是指eventpoll实例对应的文件描述符
     * 因此直接从中拿私有数据--预定义的*eventpoll
     */
	ep = file-&gt;private_data;

	mutex_lock(&amp;ep-&gt;mtx);

	/*
     * 从RB树中查找已经维护起来的监听事件
     * 当然，必须先把这个RB树结构锁定，防止查找时结构改变
	 */
	epi = ep_find(ep, tfile, fd);

	error = -EINVAL;
	switch (op) {
	case EPOLL_CTL_ADD:     // 新增监听
		if (!epi) {
			epds.events |= POLLERR | POLLHUP;

			error = ep_insert(ep, &amp;epds, tfile, fd);
		} else
			error = -EEXIST;
		break;
	case EPOLL_CTL_DEL:     // 删除监听
		if (epi)
			error = ep_remove(ep, epi);
		else
			error = -ENOENT;
		break;
	case EPOLL_CTL_MOD:     // 修改监听
		if (epi) {
			epds.events |= POLLERR | POLLHUP;
			error = ep_modify(ep, epi, &amp;epds);
		} else
			error = -ENOENT;
		break;
	}
	mutex_unlock(&amp;ep-&gt;mtx);

error_tgt_fput:
	fput(tfile);
error_fput:
	fput(file);
error_return:
	DNPRINTK(3, (KERN_INFO &quot;[%p] eventpoll: sys_epoll_ctl(%d, %d, %d, %p) = %d\n&quot;,
		     current, epfd, op, fd, event, error));

	return error;
}
</code></pre>

<p>这里的核心操作，就是往 <code>eventpoll</code> 实例中增删改监听的事件。以 <code>ep_insert</code> 为例，先看看怎么新增监听。</p>

<pre><code class="language-c">/* 实例化 epitem */
if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
    goto error_return;
/* 对 epitem 实例进行初始化数据 */
ep_rb_initnode(&amp;epi-&gt;rbn);
INIT_LIST_HEAD(&amp;epi-&gt;rdllink);
INIT_LIST_HEAD(&amp;epi-&gt;fllink);
INIT_LIST_HEAD(&amp;epi-&gt;pwqlist);
epi-&gt;ep = ep;
/* 构建struct epoll_filefd，作为rb_tree比较不同的key */
ep_set_ffd(&amp;epi-&gt;ffd, tfile, fd);
epi-&gt;event = *event;
epi-&gt;nwait = 0;
epi-&gt;next = EP_UNACTIVE_PTR;
</code></pre>

<p>接下来要接触的就是一段比较烧脑的逻辑。</p>

<p><code>poll_table</code> 是在 VFS 实现中相当重要的一个数据结构，用来与<code>poll</code>配合（这里的<code>poll</code>是指文件操作中的，而不是<code>poll()</code>系统调用）</p>

<pre><code class="language-c">typedef struct poll_table_struct {
	poll_queue_proc _qproc;
} poll_table;
</code></pre>

<p>其中<code>poll_queue_proc</code>是一个函数指针</p>

<pre><code class="language-c">typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);
</code></pre>

<p>为了让 <code>epitem</code> 更方便地追踪 <code>poll_queue_proc</code>，<code>epoll_ctl</code> 中使用了一个 <code>ep_pqueue</code> 的数据结构来包装 <code>poll_table</code>。</p>

<pre><code class="language-c">/* 使用 queue 回调函数初始化 poll table */
epq.epi = epi;
init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc);
</code></pre>

<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0qwphhnz0j30xa0o6aan.jpg" alt="" /></p>

<p>之后就是把这个 <code>poll_table</code> 作为钩子方法挂载到被监控的文件上</p>

<pre><code class="language-c">revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt);
</code></pre>

<p>这里以 <code>tcp_poll</code> 为例，先看看这段逻辑怎么实现的。</p>

<pre><code class="language-c">static unsigned int sock_poll(struct file *file, poll_table *wait)
{
    struct socket *sock;
    /* 获取 struct sock 内核套接字数据结构 */
    sock = file-&gt;private_data;
    return sock-&gt;ops-&gt;poll(file, sock, wait);
}

unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
{
	unsigned int mask;
	struct sock *sk = sock-&gt;sk;
	struct tcp_sock *tp = tcp_sk(sk);

    /*
     * 这里将调用 poll_table *wait 维护的回调函数
     * 将持有 eventpoll 实例的进程注册到 sk 的等待队列中
     */
	poll_wait(file, sk-&gt;sk_sleep, wait);
	if (sk-&gt;sk_state == TCP_LISTEN)
		return inet_csk_listen_poll(sk);

    /*
     * 这里省略了部分逻辑，主要是负责处理 struct sock 接收到的事件
     * 处理成 mask 并返回
     */
    ...

    return mask;
}

static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
{
    if (p &amp;&amp; wait_address)
        p-&gt;qproc(filp, wait_address, p);
}
</code></pre>

<p>再来看看epoll定义的回调函数的实现。</p>

<pre><code class="language-c">static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
				 poll_table *pt)
{
	struct epitem *epi = ep_item_from_epqueue(pt);
	struct eppoll_entry *pwq;

	if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
		init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback);
		pwq-&gt;whead = whead;
		pwq-&gt;base = epi;
        /* 添加到 struct sock 等待队列队首 */
		add_wait_queue(whead, &amp;pwq-&gt;wait);
		list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist);
		epi-&gt;nwait++;
	} else {
		/* We have to signal that an error occurred */
		epi-&gt;nwait = -1;
	}
}
</code></pre>

<p>到此为止，<code>ep_insert</code> 核心的逻辑已经介绍完毕。主要就是将当前的 <code>eventpoll</code> 实例注册到监听目标（文件描述符）的等待队列上，并注册<code>ep_poll_callback</code>作为回调函数。回调函数实现是怎样呢？</p>

<pre><code class="language-c">static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
	int pwake = 0;
	unsigned long flags;
    /* 获取 wait 结构维护的 epitem 实例 */
	struct epitem *epi = ep_item_from_wait(wait);
	struct eventpoll *ep = epi-&gt;ep;

    // ... code omitted...
    /* 
     * 把当前的 epitem 实例添加到 eventpoll 实例的就绪队列中 
     * 这是显然的，毕竟此回调函数只有在fd准备就绪后被回调
     */
	list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist);

is_linked:
    /* 如果 wait 结构维护的进程处于Sleeping状态，则将其唤醒并加入任务就绪队列 */
	if (waitqueue_active(&amp;ep-&gt;wq))
		__wake_up_locked(&amp;ep-&gt;wq, TASK_UNINTERRUPTIBLE |
				 TASK_INTERRUPTIBLE);
	if (waitqueue_active(&amp;ep-&gt;poll_wait))
		pwake++;

out_unlock:
	spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);

	/* We have to call this outside the lock */
	if (pwake)
		ep_poll_safewake(&amp;psw, &amp;ep-&gt;poll_wait);

	return 1;
}
</code></pre>

<h3 id="epoll-wait"><code>epoll_wait</code></h3>

<p>处理完所有的监听事件的维护，用户程序需要通过 <code>epoll_wait</code> 与 <code>eventpoll</code> 实例进行交互，并被告知所有正在监听中的事件是否发生。由于 <code>epoll_wait</code> 的整个逻辑基本上都是在进行错误检测，此处不表。我们只关注其中的核心逻辑，即调用的 <code>ep_poll</code> 函数。</p>

<pre><code class="language-c">static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
		   int maxevents, long timeout)
{
	int res, eavail;
	unsigned long flags;
	long jtimeout;
	wait_queue_t wait;

	/*
	 * Calculate the timeout by checking for the &quot;infinite&quot; value ( -1 )
	 * and the overflow condition. The passed timeout is in milliseconds,
	 * that why (t * HZ) / 1000.
	 */
	jtimeout = (timeout &lt; 0 || timeout &gt;= EP_MAX_MSTIMEO) ?
		MAX_SCHEDULE_TIMEOUT : (timeout * HZ + 999) / 1000;

retry:
	spin_lock_irqsave(&amp;ep-&gt;lock, flags);

	res = 0;
    /* 
     * 如果 eventpoll 实例的就绪队列为空，表明有没任何监听的事件发生。
     * 主动让进程陷入Sleeping状态，知道被 ep_poll_callback() 唤醒
	 */
	if (list_empty(&amp;ep-&gt;rdllist)) {
		init_waitqueue_entry(&amp;wait, current);
		wait.flags |= WQ_FLAG_EXCLUSIVE;
		__add_wait_queue(&amp;ep-&gt;wq, &amp;wait);

		for (;;) {
			/*
			 * We don't want to sleep if the ep_poll_callback() sends us
			 * a wakeup in between. That's why we set the task state
			 * to TASK_INTERRUPTIBLE before doing the checks.
			 */
			set_current_state(TASK_INTERRUPTIBLE);
			if (!list_empty(&amp;ep-&gt;rdllist) || !jtimeout)
				break;
			if (signal_pending(current)) {
				res = -EINTR;
				break;
			}

			spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);
            /* 主动陷入Sleeping状态 */
			jtimeout = schedule_timeout(jtimeout);
			spin_lock_irqsave(&amp;ep-&gt;lock, flags);
		}
		__remove_wait_queue(&amp;ep-&gt;wq, &amp;wait);

		set_current_state(TASK_RUNNING);
	}

	/* Is it worth to try to dig for events ? */
	eavail = !list_empty(&amp;ep-&gt;rdllist);

	spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);

	/*
     * 将监听到的事件拷贝到用户空间。如果没有事件就绪且还没超时，就再抱着
     * 希望试一次。
	 */
	if (!res &amp;&amp; eavail &amp;&amp;
	    !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; jtimeout)
		goto retry;

	return res;
}
</code></pre>

<h3 id="extra">Extra</h3>

<p>这部分的数据结构配合着一些胶水代码比较难以理解，最好配合着类图看看（个人使用，不一定适合各位）</p>

<p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g0rmx65dzhj324y0t8di4.jpg" alt="" /></p>

<h2 id="参考">参考</h2>

<ol>
<li>Linux Kernel 2.6.24</li>
<li><a href="https://idndx.com/2014/09/01/the-implementation-of-epoll-1/">The Implementation of epoll(1)</a></li>
<li><a href="https://idndx.com/2014/09/02/the-implementation-of-epoll-2/">The Implementation of epoll(2)</a></li>
<li><a href="https://idndx.com/2014/09/22/the-implementation-of-epoll-3/">The Implementation of epoll(3)</a></li>
<li><a href="https://idndx.com/2015/07/08/the-implementation-of-epoll-4/">The Implementation of epoll(4)</a>
<code>plain
__                    __                  
/ _| __ _ _ __   __ _ / _| ___ _ __   __ _ 
| |_ / _` | '_ \ / _` | |_ / _ \ '_ \ / _` |
|  _| (_| | | | | (_| |  _|  __/ | | | (_| |
|_|  \__,_|_| |_|\__, |_|  \___|_| |_|\__, |
             |___/                |___/ 
</code></li>
</ol>
  </div>
  
  <div class="pagination">
    <div class="pagination__title">
      <span
        class="pagination__title-h">Read other posts</span>
      <hr />
    </div>
    <div class="pagination__buttons">
      
      <span class="button previous">
        <a href="https://ffutop.github.io/posts/2019-03-19-janus/">
          <span class="button__icon">←</span>
          <span class="button__text">认证与授权·简述</span>
        </a>
      </span>
      
      
      <span class="button next">
        <a href="https://ffutop.github.io/posts/2019-02-27-java-fatal-api/">
          <span class="button__text">【Java】API 参数误定义的后果</span>
          <span class="button__icon">→</span>
        </a>
      </span>
      
    </div>
  </div>
  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright copyright--user">
        <span>2018-2019 © ffutop</span>
        <span>:: Powered by <a href="http://gohugo.io">Hugo</a></span>
      </div>
    
  </div>
</footer>

<script src="https://ffutop.github.io/assets/main.js"></script>
<script src="https://ffutop.github.io/assets/prism.js"></script>


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
    processEscapes: true
  }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>






  
</div>

</body>
</html>
