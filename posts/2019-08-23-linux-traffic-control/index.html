<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.55.6" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="ffutop" />
  <meta property="og:url" content="https://www.ffutop.com/posts/2019-08-23-linux-traffic-control/" />
  <link rel="canonical" href="https://www.ffutop.com/posts/2019-08-23-linux-traffic-control/" /><link rel="apple-touch-icon" href="favicon.ico" />
  <link rel="icon" href="favicon.ico" />
  <link rel="shortcut" href="favicon.ico" /><link rel="alternate" type="application/atom+xml" href="https://www.ffutop.com/index.xml" title="Utop&#39;s Blog">

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/www.ffutop.com\/"
      },
      "articleSection" : "posts",
      "name" : "Linux Traffic Control",
      "headline" : "Linux Traffic Control",
      "description" : "\x3cp\x3e最近在处理 Kubernetes 工作的时候，被问及这样一个命题：Pod 能对 CPU 和内存施加限制，那同样属于资源范畴的网络带宽是否能限制呢？使用 Kubernetes 的一个核心优势在于每个 Pod 都等同于一个轻量级的“操作系统”。建立在 Linux 命名空间(namespace)和控制组(cgroups)基础上的容器技术将每个 Pod 的资源进行了隔离和限制。但是，限流只针对 CPU 和内存，对网络、磁盘 IO 的解决方案仅仅局限在隔离，难道技术上实现不了吗？自然不是，Kubernetes 有意识地将网络模块拆解，只定义插件规范，而将实现的可能性交由下游开发自由决策。当然，本篇并不在意 Kubernetes 网络限流的解决方案，只是以此作为引子。\x3c\/p\x3e\n\n\x3cp\x3e流量控制(Traffic Control, TC) 是 Linux 内核提供的流量限速、整形、策略控制机制。近乎完美地支持网络限流的命题，除了，这是比 Netfilter 更难理解的模块。Netfilter 作用在内核网络协议栈上，通过在各个枢纽设立关卡对网络包(\x3ccode\x3esk_buff\x3c\/code\x3e 数据结构, skb)进行检查，并实施 ACCEPT、DROP、MASQUERADE 等策略。相比之下，TC 是绑定在网络设备上实施的。提供 \x3ccode\x3eenqueue\x3c\/code\x3e, \x3ccode\x3edequeue\x3c\/code\x3e 两个核心函数，也是作为关卡对到达网络设备的网络包实施策略。要说核心的不同之处，Netfilter 是流式地处理网络包，先到的网络包一定先出（也可能是被丢弃）；TC 的处理方式就依照策略，类比块设备的随机读\/随机写了。\x3c\/p\x3e",
      "inLanguage" : "zh-cmn-Hans-CN",
      "author" : "ffutop",
      "creator" : "ffutop",
      "publisher": "ffutop",
      "accountablePerson" : "ffutop",
      "copyrightHolder" : "ffutop",
      "copyrightYear" : "23230",
      "datePublished": "2019-08-23 00:00:00 \x2b0000 UTC",
      "dateModified" : "2019-08-23 00:00:00 \x2b0000 UTC",
      "url" : "https:\/\/www.ffutop.com\/posts\/2019-08-23-linux-traffic-control\/",
      "keywords" : [ "Linux","Kernel","TC", ]
  }
</script>
<title>Linux Traffic Control - Utop&#39;s Blog</title>
  <meta property="og:title" content="Linux Traffic Control - Utop&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:description" content="最近在处理 Kubernetes 工作的时候，被问及这样一个命题：Pod 能对 CPU 和内存施加限制，那同样属于资源范畴的网络带宽是否能限制呢？使用 Kubernetes 的一个核心优势在于每个 Pod 都等同于一个轻量级的“操作系统”。建立在 Linux 命名空间(namespace)和控制组(cgroups)基础上的容器技术将每个 Pod 的资源进行了隔离和限制。但是，限流只针对 CPU 和内存，对网络、磁盘 IO 的解决方案仅仅局限在隔离，难道技术上实现不了吗？自然不是，Kubernetes 有意识地将网络模块拆解，只定义插件规范，而将实现的可能性交由下游开发自由决策。当然，本篇并不在意 Kubernetes 网络限流的解决方案，只是以此作为引子。

流量控制(Traffic Control, TC) 是 Linux 内核提供的流量限速、整形、策略控制机制。近乎完美地支持网络限流的命题，除了，这是比 Netfilter 更难理解的模块。Netfilter 作用在内核网络协议栈上，通过在各个枢纽设立关卡对网络包(sk_buff 数据结构, skb)进行检查，并实施 ACCEPT、DROP、MASQUERADE 等策略。相比之下，TC 是绑定在网络设备上实施的。提供 enqueue, dequeue 两个核心函数，也是作为关卡对到达网络设备的网络包实施策略。要说核心的不同之处，Netfilter 是流式地处理网络包，先到的网络包一定先出（也可能是被丢弃）；TC 的处理方式就依照策略，类比块设备的随机读/随机写了。" />
  <meta name="description" content="最近在处理 Kubernetes 工作的时候，被问及这样一个命题：Pod 能对 CPU 和内存施加限制，那同样属于资源范畴的网络带宽是否能限制呢？使用 Kubernetes 的一个核心优势在于每个 Pod 都等同于一个轻量级的“操作系统”。建立在 Linux 命名空间(namespace)和控制组(cgroups)基础上的容器技术将每个 Pod 的资源进行了隔离和限制。但是，限流只针对 CPU 和内存，对网络、磁盘 IO 的解决方案仅仅局限在隔离，难道技术上实现不了吗？自然不是，Kubernetes 有意识地将网络模块拆解，只定义插件规范，而将实现的可能性交由下游开发自由决策。当然，本篇并不在意 Kubernetes 网络限流的解决方案，只是以此作为引子。

流量控制(Traffic Control, TC) 是 Linux 内核提供的流量限速、整形、策略控制机制。近乎完美地支持网络限流的命题，除了，这是比 Netfilter 更难理解的模块。Netfilter 作用在内核网络协议栈上，通过在各个枢纽设立关卡对网络包(sk_buff 数据结构, skb)进行检查，并实施 ACCEPT、DROP、MASQUERADE 等策略。相比之下，TC 是绑定在网络设备上实施的。提供 enqueue, dequeue 两个核心函数，也是作为关卡对到达网络设备的网络包实施策略。要说核心的不同之处，Netfilter 是流式地处理网络包，先到的网络包一定先出（也可能是被丢弃）；TC 的处理方式就依照策略，类比块设备的随机读/随机写了。" />
  <meta property="og:locale" content="zh-cmn-Hans-CN" />

  <link rel="stylesheet" href="/css/flexboxgrid-6.3.1.min.css" />
  <link rel="stylesheet" href="/css/github-markdown.css" />
  <link rel="stylesheet" href="/css/highlight/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml"
    title="Utop&#39;s Blog">
  
  <link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker|Bree+Serif" rel="stylesheet">
  
  

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-92258941-1"></script>
</head>


<body>
  <article class="post Chinese" id="article">
    <div class="row">
      <div class="col-xs-12">
        <div class="site-header">
          
<header>
  <div class="signatures site-title">
    <a href="/">FFUTOP</a>
  </div>
</header>
<div class="row end-xs">
  
  
</div>
<div class="header-line"></div>

        </div>
        <header class="post-header">
          <h1 class="post-title">Linux Traffic Control</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2019-08-23 00:00:00 UTC">
                23 Aug 2019
              </time>
              
            </div>
            <div class="col-xs-6">
              
              <div class="post-author">
                <a target="_blank" href="https://www.ffutop.com/">@ffutop</a>
              </div>
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          
          <p>最近在处理 Kubernetes 工作的时候，被问及这样一个命题：Pod 能对 CPU 和内存施加限制，那同样属于资源范畴的网络带宽是否能限制呢？使用 Kubernetes 的一个核心优势在于每个 Pod 都等同于一个轻量级的“操作系统”。建立在 Linux 命名空间(namespace)和控制组(cgroups)基础上的容器技术将每个 Pod 的资源进行了隔离和限制。但是，限流只针对 CPU 和内存，对网络、磁盘 IO 的解决方案仅仅局限在隔离，难道技术上实现不了吗？自然不是，Kubernetes 有意识地将网络模块拆解，只定义插件规范，而将实现的可能性交由下游开发自由决策。当然，本篇并不在意 Kubernetes 网络限流的解决方案，只是以此作为引子。</p>

<p>流量控制(Traffic Control, TC) 是 Linux 内核提供的流量限速、整形、策略控制机制。近乎完美地支持网络限流的命题，除了，这是比 Netfilter 更难理解的模块。Netfilter 作用在内核网络协议栈上，通过在各个枢纽设立关卡对网络包(<code>sk_buff</code> 数据结构, skb)进行检查，并实施 ACCEPT、DROP、MASQUERADE 等策略。相比之下，TC 是绑定在网络设备上实施的。提供 <code>enqueue</code>, <code>dequeue</code> 两个核心函数，也是作为关卡对到达网络设备的网络包实施策略。要说核心的不同之处，Netfilter 是流式地处理网络包，先到的网络包一定先出（也可能是被丢弃）；TC 的处理方式就依照策略，类比块设备的随机读/随机写了。</p>

<h2 id="前置知识-发送网络包流程">前置知识：发送网络包流程</h2>

<p>此处提供一个内核网络包的发送流程，辅助了解网络协议栈。<em>跳过不影响后文阅读</em></p>

<p>网络包的发送由应用层通过系统调用 <code>sendmsg</code> (类似的系统调用还有 <code>send</code>, <code>sendto</code> 等) 发起，后陷入内核态，由内核代码实现网络协议栈逐层封装，逐层下发的工作。</p>

<pre><code class="language-plain"> 0)               |              sock_sendmsg() {       /* 系统调用 sendmsg 对应的内核函数 */
 0)               |                inet_sendmsg() {     /* 使用 IPv4 (not IPv6) 实现的 sendmsg */
 0)               |                  tcp_sendmsg() {    /* 传输层，TCP 协议的处理 */
 0)               |                      sk_stream_alloc_skb() {    /* 创建 sk_buff(skb) 数据结构，网络包在内核中存在的形式 */
 0)               |                        __alloc_skb() {
 0)   5.273 us    |                        }
 0)   0.056 us    |                        sk_forced_mem_schedule();
 0)   6.026 us    |                      }
 0)               |                      tcp_push() {
 0)               |                        __tcp_push_pending_frames() {
 0)               |                          tcp_write_xmit() {
 0)   0.132 us    |                            tcp_tso_segs();
 0)   0.058 us    |                            tcp_init_tso_segs();
 0)               |                            tcp_transmit_skb() { /* 下发 skb ，交由网络层继续 */
 0)   0.046 us    |                              skb_push();
 0)               |                              ip_queue_xmit() {  /* 网络层，IP 协议的处理 */
 0)               |                                __sk_dst_check() {
 0)   0.069 us    |                                  ipv4_dst_check();
 0)   0.429 us    |                                }
 0)   0.049 us    |                                skb_push();
 0)               |                                  ip_output() {
 0)               |                                    ip_finish_output() {
 0)               |                                      ip_finish_output2() {
 0)   0.044 us    |                                        skb_push();
 0)               |                                        dev_queue_xmit() {   /* 数据链路层，由网卡设备（内核根据物理设备抽象出来的概念）继续处理 */
 0)               |                                          __dev_queue_xmit() {
 0)   0.151 us    |                                            netdev_pick_tx();
 0)   0.046 us    |                                            _raw_spin_lock();
 0)               |                                            /* some important things omitted */
 0) + 39.674 us   |                                          }
 0) + 40.053 us   |                                        }
 0) + 41.656 us   |                                      }
 0) + 43.041 us   |                                    }
 0) + 47.121 us   |                                  }
 0) + 68.614 us   |                                }
 0) + 70.558 us   |                              }
 0) + 75.995 us   |                            }
 0) + 85.718 us   |                          }
 0) + 86.418 us   |                        }
 0) + 87.275 us   |                      }
 0) ! 101.636 us  |                    }
 0) ! 105.024 us  |                  }
 0) ! 105.516 us  |                }
 0) ! 107.861 us  |              }
 0) ! 108.230 us  |            }
 0) ! 108.653 us  |          }
 0) ! 109.066 us  |        }
 0) ! 112.023 us  |      }
 0) ! 113.078 us  |    }
 0) ! 113.374 us  |  }
</code></pre>

<p>本篇核心关注流量控制，就不细数网络协议栈了。待到网络包(skb)到达数据链路层，<code>dev_queue_xmit</code> 标志着 skb 终于交付给网络设备，下面就是 TC 策略的运转了。</p>

<p>Qdisc (queueing discipline) ，是整个 TC 的基本模型。所有需要通过网卡接口发送的数据包，都会进入接口绑定的 Qdisc 等待队列(enqueue)。再由 ksoftirq 内核线程读取接口 Qdisc 中的数据包(dequeue)，尽最大能力发送出去。</p>

<h2 id="traffic-control-原理">Traffic Control 原理</h2>

<p>对于 TC 核心的概念——队列规程，类别，过滤器，网上充斥着大量的资料，由于没有自信讲得更加通透，附上链接一枚。</p>

<p><a href="https://blog.csdn.net/dog250/article/details/40483627">https://blog.csdn.net/dog250/article/details/40483627</a></p>

<h2 id="traffic-control-实现">Traffic Control 实现</h2>

<h3 id="数据结构">数据结构</h3>

<p>遵循 TC Qdisc 的生命周期，首先要介绍的是 Qdisc 的创建流程。不过在此之前，看了解下相关的数据结构。</p>

<p><img src="https://img.ffutop.com/D4466735-ED98-4A94-9C7A-FD59F7791988.png" alt="" /></p>

<pre><code class="language-c">struct net_device
{
    /* 设备的发送队列列表 (这是数组的头指针)*/
    struct netdev_queue *_tx;

    /* 发送队列的个数 */
    unsigned int num_tx_queues;

    /* 使用中的发送队列个数 */
    unsigned int real_num_tx_queues;

    /* 默认的队列策略 */
    struct Qdisc *qdisc;

    /* 每个发送队列的最大长度 */
    unsigned long tx_queue_len;

    /* 发送队列的全局锁 */
    spinlock_t tx_global_lock;
}

struct netdev_queue {
    /* 所属网络设备 */
    struct net_device *dev;
    /* 队列相应的队列策略 */
    struct Qdisc *qdisc;
    struct Qdisc *qdisc_sleeping;
    spinlock_t _xmit_lock;
    int            xmit_lock_owner;
    unsigned long        trans_start;
    unsigned long        trans_timeout;
    /* 队列状态 */
    unsigned long        state;
};

#define TCQ_F_BUILTIN        1
#define TCQ_F_INGRESS        2
#define TCQ_F_CAN_BYPASS    4
#define TCQ_F_MQROOT        8
#define TCQ_F_ONETXQUEUE    0x10
#define TCQ_F_WARN_NONWC    (1 &lt;&lt; 16)

struct Qdisc {
    /* skb 入队函数 */
    int (*enqueue)(struct sk_buff *skb, struct Qdisc *dev);
    /* skb 出队函数 */
    struct sk_buff * (*dequeue)(struct Qdisc *dev);
    unsigned int flags;
    struct list_head    list;
    int padded;
    /* 队列策略函数集 */
    const struct Qdisc_ops *ops;
    int            (*reshape_fail)(struct sk_buff *skb,
                    struct Qdisc *q);
    /* 所属设备队列 */
    struct netdev_queue *dev_queue;
    struct Qdisc *next_sched;

    /* 队列策略的状态 */
    unsigned long state;
    struct sk_buff_head q;
    u32 limit;
};
</code></pre>

<h3 id="网络设备初始化与-qdisc">网络设备初始化与 Qdisc</h3>

<p>TC 的实施与网络设备密切相关。跳过设备的注册和创建流程，设备状态由 <code>state DOWN</code> 切换到 <code>state UP</code> 标志着设备正式启用</p>

<p><img src="https://img.ffutop.com/BF9E5268-8C81-499D-82B6-20DA7DF57473.png" alt="" /></p>

<p>对应到内核流程，调用栈类似：</p>

<pre><code class="language-c">dev_open
|-- __dev_open
    |-- dev_active
        |-- attach_default_qdiscs

static void attach_default_qdiscs(struct net_device *dev)
{
    struct netdev_queue *txq;
    struct Qdisc *qdisc;

    /* 获得设备的第 0 个 queue */
    txq = netdev_get_tx_queue(dev, 0);

    /* 如果发送队列个数 &lt;= 1 || 发送队列长度 = 0 */
    if (!netif_is_multiqueue(dev) || dev-&gt;tx_queue_len == 0) {
        /* 单队列的流量控制 */
        netdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);
        dev-&gt;qdisc = txq-&gt;qdisc_sleeping;
        atomic_inc(&amp;dev-&gt;qdisc-&gt;refcnt);
    } else {
        /* 多队列的流量控制; 此处 mq 指 multiqueue*/
        qdisc = qdisc_create_dflt(txq, &amp;mq_qdisc_ops, TC_H_ROOT);
        if (qdisc) {
            qdisc-&gt;ops-&gt;attach(qdisc);
            dev-&gt;qdisc = qdisc;
        }
    }
}

static void attach_one_default_qdisc(struct net_device *dev,
                     struct netdev_queue *dev_queue,
                     void *_unused)
{
    /* noqueue 默认的，无队列控制实现，一般应用在虚拟设备上 */
    struct Qdisc *qdisc = &amp;noqueue_qdisc;

    /* 如果有发送队列长度限制 (默认应用在物理网卡设备上) */
    if (dev-&gt;tx_queue_len) {
        /* 创建 快速优先队列 策略的 Qdisc */
        qdisc = qdisc_create_dflt(dev_queue,
                      &amp;pfifo_fast_ops, TC_H_ROOT);
    }
    dev_queue-&gt;qdisc_sleeping = qdisc;
}
</code></pre>

<h3 id="qdisc-的生命周期">Qdisc 的生命周期</h3>

<p>应用程序发送数据包都将涉及到系统调用，一般来说是 <code>sendmsg</code>、<code>sendto</code> 之类的系统调用。上面👆给了内核 <code>sendmsg</code> 的函数调用链路。很长，涉及的操作相当多。但是，这还仅仅是由 CPU 执行的一部分流程。试想，还有内存到网卡设备的数据交互。这里等待的时间将更加漫长，更不要说像是网络重试之类的实现了。内核究竟做了怎样的实现呢？在<a href="https://www.ffutop.com/posts/2019-01-15-understand-kernel-8/">这篇网络</a>中，描述了接收网络数据时的三阶段流程。通过接收队列作为中介，<code>recv</code> 系统调用需要做的只是去检测接收队列是否有数据包，其它流程全部交由内核线程和网络硬中断完成。类似的，发送数据包 <code>sendmsg</code> 这类系统调用，也只是将数据包提交到一个等待队列，再由内核线程和中断实施尽力发送。这里的等待队列，就是网络设备绑定的 Qdisc。</p>

<p>先别管 Qdisc 是怎么实现的，它提供了两个核心函数 <code>enqueue</code>, <code>dequeue</code> ，完全符合定义一个队列的基本要求。</p>

<p>额外地，一些函数 <code>peek</code>, <code>reset</code> 等，作为这个黑盒的辅助函数，在个别情况下进行使用。</p>

<ul>
<li><p>peek: 类似 dequeue，但获取的网络包仍然保留在队列中</p></li>

<li><p>reset: 将 Qdisc 重置到初始状态</p></li>

<li><p>init: 初始化一个新创建的 Qdisc</p></li>

<li><p>destroy: 销毁一个 Qdisc 生命周期中所使用的资源</p></li>

<li><p>change: 修改 Qdisc 的参数</p></li>
</ul>

<p>Qdisc 的整个生命周期，最初和最末分别是向内核注册一种新的 Qdisc 和取消注册一种 Qdisc</p>

<pre><code class="language-c">/* from net/sched/sch_api.c */
int register_qdisc(struct Qdisc_ops *qops)
int unregister_qdisc(struct Qdisc_ops *qops)
</code></pre>

<p>再就是创建一个 Qdisc 实例绑定到网络设备，以及从网络设备上解绑 Qdisc 实例</p>

<pre><code class="language-c">static struct Qdisc *
qdisc_create(struct net_device *dev, struct netdev_queue *dev_queue,
	     struct Qdisc *p, u32 parent, u32 handle,
	     struct nlattr **tca, int *errp)

// n-&gt;nlmsg_type == RTM_DELQDISC
static int tc_get_qdisc(struct sk_buff *skb, struct nlmsghdr *n)
</code></pre>

<p>再就是正常的工作流程了，<code>enqueue</code>, <code>dequeue</code> 以及其它辅助函数的使用。</p>

<h3 id="qdisc-执行流">Qdisc 执行流</h3>

<p><code>noop</code> 是 Qdisc 最简单，最无赖的一种实现，对于所有的数据包直接抛弃。当然，这种实现正常情况下只会出现在网络设备状态为 DOWN 的时候。</p>

<pre><code class="language-c">static int noop_enqueue(struct sk_buff *skb, struct Qdisc * qdisc)
{
    kfree_skb(skb);
    return NET_XMIT_CN;
}

static struct sk_buff *noop_dequeue(struct Qdisc * qdisc)
{
    return NULL;
}
</code></pre>

<p><code>pfifo_fast</code> 是普遍使用一种 Qdisc 实现，根据数据包 Tos 将网络包划分到三个通道，进入第0通道的网络包有 dequeue 的最高优先级。</p>

<pre><code class="language-c">static int pfifo_fast_enqueue(struct sk_buff *skb, struct Qdisc *qdisc)
{
    // 检测 Qdisc 队列数据包数量是否达到 dev 预定的最大值
    if (skb_queue_len(&amp;qdisc-&gt;q) &lt; qdisc_dev(qdisc)-&gt;tx_queue_len) {
        // 确定数据包需要进入哪个通道
        int band = prio2band[skb-&gt;priority &amp; TC_PRIO_MAX];
        struct pfifo_fast_priv *priv = qdisc_priv(qdisc);
        // 获取通道列表的 head
        struct sk_buff_head *list = band2list(priv, band);

        priv-&gt;bitmap |= (1 &lt;&lt; band);
        qdisc-&gt;q.qlen++;
        // 添加到通道队尾
        return __qdisc_enqueue_tail(skb, qdisc, list);
    }

    return qdisc_drop(skb, qdisc);
}

static struct sk_buff *pfifo_fast_dequeue(struct Qdisc *qdisc)
{
    struct pfifo_fast_priv *priv = qdisc_priv(qdisc);
    int band = bitmap2band[priv-&gt;bitmap];

    if (likely(band &gt;= 0)) {
        struct sk_buff_head *list = band2list(priv, band);
        struct sk_buff *skb = __qdisc_dequeue_head(qdisc, list);

        qdisc-&gt;q.qlen--;
        if (skb_queue_empty(list))
            priv-&gt;bitmap &amp;= ~(1 &lt;&lt; band);

        return skb;
    }

    return NULL;
}
</code></pre>

<p>至于类别队列，首先也还是 Qdisc，符合它的所有要素，只是在 <code>Qdisc_ops</code> 中加入了分类的流程。例如 htb 的实现中，额外地加入了 <code>class_ops</code>，对关于分类操作的方法做了相关的实现。</p>

<pre><code class="language-c">static struct Qdisc_ops htb_qdisc_ops __read_mostly = {
    .cl_ops    =    &amp;htb_class_ops,
    .id        =    &quot;htb&quot;,
    .priv_size =    sizeof(struct htb_sched),
    .enqueue   =    htb_enqueue,
    .dequeue   =    htb_dequeue,
    .peek      =    qdisc_peek_dequeued,
    .drop      =    htb_drop,
    .init      =    htb_init,
    .reset     =    htb_reset,
    .destroy   =    htb_destroy,
    .dump      =    htb_dump,
    .owner     =    THIS_MODULE,
};
static const struct Qdisc_class_ops htb_class_ops = {
    .graft       =    htb_graft,
    .leaf        =    htb_leaf,
    .qlen_notify =    htb_qlen_notify,
    .get         =    htb_get,
    .put         =    htb_put,
    .change      =    htb_change_class,
    .delete      =    htb_delete,
    .walk        =    htb_walk,
    .tcf_chain   =    htb_find_tcf,
    .bind_tcf    =    htb_bind_filter,
    .unbind_tcf  =    htb_unbind_filter,
    .dump        =    htb_dump_class,
    .dump_stats  =    htb_dump_class_stats,
}
</code></pre>
        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>
        <div class="row">
          <div class="col-xs-12">
            
          </div>
        </div>

        
        

<div class="releated-content">
  <h3>Related Posts</h3>
  <ul>
    
    <li><a href="/posts/2019-08-06-netfilter-%E5%AF%BC%E8%A7%88-based-on-iptables/">Netfilter 导览 - based on iptables</a></li>
    
    <li><a href="/posts/2019-07-17-%E7%90%86%E8%A7%A3-linux-kernel-13-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98/">理解 Linux Kernel (13) - 虚拟内存</a></li>
    
    <li><a href="/posts/2019-06-18-%E7%90%86%E8%A7%A3-linux-kernel-12-linux-%E5%AE%B9%E5%99%A8%E5%8C%96%E6%8A%80%E6%9C%AF/">理解 Linux Kernel (12) - Linux 容器化技术</a></li>
    
  </ul>
</div>

        
        
        <div style="height: 50px;"></div>
        
        <div class="post-comments">
          <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://ffutop.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

        </div>
        
        

        <div class="site-footer">
  
  <div class="site-footer-item">
    <a href="/index.xml" target="_blank">RSS</a>
  </div>
  
  <div class="site-footer-item">
    <a href="https://github.com/ffutop" target="_blank">Github</a>
  </div>
  
  <div class="site-footer-item">
    <a href="/about/" target="_blank">About</a>
  </div>
  
  
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  
  
    
    
  
</script>

  

</body>

</html>
